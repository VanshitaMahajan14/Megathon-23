{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "201f57c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "347b20f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0e7a50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain import PromptTemplate\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.llms import CTransformers\n",
    "from langchain.chains import RetrievalQA\n",
    "import chainlit as cl\n",
    "\n",
    "DB_FAISS_PATH = 'vectorstore/db_faiss'\n",
    "\n",
    "custom_prompt_template = \"\"\"Use the following pieces of information to answer the user's question.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Only return the helpful answer below and nothing else.\n",
    "Helpful answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fb7e0da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-29 03:50:59 - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "128186b41f2440638c0eaf77f733589a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter \n",
    "\n",
    "DATA_PATH = 'final/'\n",
    "DB_FAISS_PATH = 'new_vectorstore/db_faiss'\n",
    "\n",
    "# Create vector database\n",
    "def create_vector_db():\n",
    "    loader = DirectoryLoader(DATA_PATH,\n",
    "                             glob='*.pdf',\n",
    "                             loader_cls=PyPDFLoader)\n",
    "\n",
    "    documents = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=5000,\n",
    "                                                   chunk_overlap=50)\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "    texts = texts[:10]\n",
    "\n",
    "    embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2',\n",
    "                                       model_kwargs={'device': 'cpu'})\n",
    "\n",
    "    db = FAISS.from_documents(texts, embeddings)\n",
    "    db.save_local(DB_FAISS_PATH)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_vector_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f54bd738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7867\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7867/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-29 04:33:46 - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f902d0cac2a43789b9786acf049b183",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/gradio/routes.py\", line 422, in run_predict\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/gradio/blocks.py\", line 1323, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/gradio/blocks.py\", line 1051, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/anyio/to_thread.py\", line 31, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 937, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 867, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/16/36nvykh51lj8191tr2wrzc300000gn/T/ipykernel_4693/2815207940.py\", line 65, in final_result\n",
      "    return response[0]\n",
      "           ~~~~~~~~^^^\n",
      "KeyError: 0\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain import PromptTemplate\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.llms import CTransformers\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "DB_FAISS_PATH = 'new_vectorstore/db_faiss'\n",
    "\n",
    "custom_prompt_template = \"\"\"Use the following pieces of information to answer the user's question.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Only return the helpful answer below and nothing else.\n",
    "Helpful answer:\n",
    "\"\"\"\n",
    "\n",
    "def set_custom_prompt():\n",
    "    \"\"\"\n",
    "    Prompt template for QA retrieval for each vectorstore\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate(template=custom_prompt_template,\n",
    "                            input_variables=['context', 'question'])\n",
    "    return prompt\n",
    "\n",
    "#Retrieval QA Chain\n",
    "def retrieval_qa_chain(llm, prompt, db):\n",
    "    qa_chain = RetrievalQA.from_chain_type(llm=llm,\n",
    "                                       chain_type='stuff',\n",
    "                                       retriever=db.as_retriever(search_kwargs={'k': 2}),\n",
    "                                       return_source_documents=True,\n",
    "                                       chain_type_kwargs={'prompt': prompt}\n",
    "                                       )  \n",
    "    return qa_chain\n",
    "\n",
    "#Loading the model\n",
    "def load_llm():\n",
    "    # Load the locally downloaded model here\n",
    "    llm = CTransformers(\n",
    "        model = \"/Users/ashnadua/Desktop/Megathon/models/llama-2-7b-chat.ggmlv3.q8_0.bin\",\n",
    "        model_type=\"llama\",\n",
    "        max_new_tokens = 512,\n",
    "        temperature = 0.5\n",
    "    )\n",
    "    return llm\n",
    "\n",
    "#QA Model Function\n",
    "def qa_bot():\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "                                       model_kwargs={'device': 'cpu'})\n",
    "    db = FAISS.load_local(DB_FAISS_PATH, embeddings)\n",
    "    llm = load_llm()\n",
    "    qa_prompt = set_custom_prompt()\n",
    "    qa = retrieval_qa_chain(llm, qa_prompt, db)\n",
    "\n",
    "    return qa\n",
    "\n",
    "#output function\n",
    "def final_result(query):\n",
    "    qa_result = qa_bot()\n",
    "    response = qa_result({'query': query})\n",
    "    return response[0]\n",
    "\n",
    "iface = gr.Interface(fn=final_result, inputs=\"text\", outputs=\"text\")\n",
    "iface.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27526d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8560773f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce74106",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db96b6fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff536c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddd7d3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692d4f98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a92c5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f0a233",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81bb480",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fcbdf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e661f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
